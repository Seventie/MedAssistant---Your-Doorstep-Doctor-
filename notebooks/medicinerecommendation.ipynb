{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Medicine recommendation RAG pipeline\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL FAISS\n",
    "# -------------------------\n",
    "USE_FAISS = True\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    USE_FAISS = False\n",
    "\n",
    "# -------------------------\n",
    "# OPTIONAL GROQ CLIENT\n",
    "# -------------------------\n",
    "try:\n",
    "    from groq import Groq\n",
    "except ImportError:\n",
    "    Groq = None\n",
    "    print(\"[WARN] groq SDK not installed.\")\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "OUT_DIR = Path(\"kg_rag_artifacts\")\n",
    "DATA_CSV = \"drugs_side_effects.csv\"\n",
    "EMBEDDING_FILE = OUT_DIR / \"corpus_embeddings.npy\"\n",
    "FAISS_INDEX_FILE = OUT_DIR / \"faiss.index\"\n",
    "KG_FILE = OUT_DIR / \"medical_kg.graphml\"\n",
    "\n",
    "EMBEDDER_MODEL = \"all-MiniLM-L6-v2\"\n",
    "GROQ_MODEL = \"gemma2-9b-it\"\n",
    "\n",
    "# âœ… FIX: read API key from environment\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = re.sub(r\"[\\r\\n]+\", \" \", s)\n",
    "    s = re.sub(r\"[^A-Za-z0-9\\s\\-,\\.;:()/%]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# -------------------------\n",
    "# LOAD NLP\n",
    "# -------------------------\n",
    "try:\n",
    "    import scispacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_sci_sm\")\n",
    "    except Exception:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "embedder = SentenceTransformer(EMBEDDER_MODEL)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_CSV).fillna(\"\")\n",
    "\n",
    "for col in [\"drug_name\", \"side_effects\", \"medical_condition\"]:\n",
    "    df[f\"{col}_clean\"] = df[col].astype(str).apply(clean_text)\n",
    "\n",
    "corpus_embeddings = np.load(EMBEDDING_FILE)\n",
    "\n",
    "# FAISS\n",
    "if USE_FAISS and FAISS_INDEX_FILE.exists():\n",
    "    index = faiss.read_index(str(FAISS_INDEX_FILE))\n",
    "else:\n",
    "    index = None\n",
    "    print(\"[WARN] FAISS not available, using brute-force similarity\")\n",
    "\n",
    "# Knowledge Graph\n",
    "G = nx.read_graphml(KG_FILE)\n",
    "\n",
    "# -------------------------\n",
    "# NER\n",
    "# -------------------------\n",
    "def run_ner(text):\n",
    "    doc = nlp(text)\n",
    "    ents = [(ent.text.strip(), ent.label_) for ent in doc.ents]\n",
    "    if not ents:\n",
    "        ents = [(chunk.text.strip(), \"NOUN_CHUNK\") for chunk in doc.noun_chunks]\n",
    "    return list(dict.fromkeys(ents))\n",
    "\n",
    "def extract_query_entities(symptoms, additional_info):\n",
    "    tokens = [clean_text(s) for s in symptoms]\n",
    "    tokens += [clean_text(e) for e, _ in run_ner(additional_info)]\n",
    "\n",
    "    doc = nlp(additional_info)\n",
    "    for tok in doc:\n",
    "        if tok.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\"} and len(tok.text) > 2:\n",
    "            tokens.append(clean_text(tok.text))\n",
    "\n",
    "    return list(dict.fromkeys(filter(None, tokens)))\n",
    "\n",
    "# -------------------------\n",
    "# KG HELPERS\n",
    "# -------------------------\n",
    "def match_graph_nodes(tokens, max_matches=10):\n",
    "    matches = []\n",
    "    for t in tokens:\n",
    "        for n, d in G.nodes(data=True):\n",
    "            if t.lower() in d.get(\"label\", \"\").lower():\n",
    "                matches.append(n)\n",
    "                if len(matches) >= max_matches:\n",
    "                    break\n",
    "    return list(dict.fromkeys(matches))\n",
    "\n",
    "def expand_subgraph(seed_nodes, radius=2):\n",
    "    if not seed_nodes:\n",
    "        return nx.Graph()\n",
    "\n",
    "    nodes = set(seed_nodes)\n",
    "    frontier = set(seed_nodes)\n",
    "\n",
    "    for _ in range(radius):\n",
    "        new = set()\n",
    "        for n in frontier:\n",
    "            new |= set(G.successors(n)) | set(G.predecessors(n))\n",
    "        nodes |= new\n",
    "        frontier = new\n",
    "\n",
    "    return G.subgraph(nodes).copy()\n",
    "\n",
    "def subgraph_to_text(subg, max_triples=60):\n",
    "    triples = []\n",
    "    for u, v, d in subg.edges(data=True):\n",
    "        triples.append(\n",
    "            f\"{subg.nodes[u].get('label', u)} --{d.get('relation','related_to')}--> {subg.nodes[v].get('label', v)}\"\n",
    "        )\n",
    "    return \"\\n\".join(triples[:max_triples])\n",
    "\n",
    "# -------------------------\n",
    "# SEMANTIC SEARCH\n",
    "# -------------------------\n",
    "def semantic_retrieve(text, top_k=5):\n",
    "    qv = embedder.encode([clean_text(text)], normalize_embeddings=True)\n",
    "\n",
    "    if USE_FAISS and index is not None:\n",
    "        _, I = index.search(qv.astype(\"float32\"), top_k)\n",
    "        idx = I[0]\n",
    "    else:\n",
    "        sims = cosine_similarity(qv, corpus_embeddings)[0]\n",
    "        idx = sims.argsort()[-top_k:][::-1]\n",
    "\n",
    "    return df.iloc[idx].copy()\n",
    "\n",
    "# -------------------------\n",
    "# GROQ GENERATION\n",
    "# -------------------------\n",
    "def generate_with_groq(question, context):\n",
    "    if Groq is None:\n",
    "        raise RuntimeError(\"Groq SDK not installed\")\n",
    "    if not GROQ_API_KEY:\n",
    "        raise RuntimeError(\"GROQ_API_KEY not set\")\n",
    "\n",
    "    client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=GROQ_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# -------------------------\n",
    "# MAIN ORCHESTRATOR\n",
    "# -------------------------\n",
    "def answer_via_kg_and_semantics(symptoms, additional_info, question):\n",
    "    tokens = extract_query_entities(symptoms, additional_info)\n",
    "    seeds = match_graph_nodes(tokens)\n",
    "    subg = expand_subgraph(seeds)\n",
    "    context = subgraph_to_text(subg)\n",
    "\n",
    "    if GROQ_API_KEY:\n",
    "        return generate_with_groq(question, context)\n",
    "    return context\n",
    "\n",
    "# -------------------------\n",
    "# EXAMPLE\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    symptoms = [\"Fever\", \"Fatigue\"]\n",
    "    info = \"Mild fever and headache for two days.\"\n",
    "    q = \"Which OTC drugs are safe?\"\n",
    "\n",
    "    print(answer_via_kg_and_semantics(symptoms, info, q))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
