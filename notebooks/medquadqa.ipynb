{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 0 — INSTALL ALL DEPENDENCIES UP FRONT\n",
    "-------------------------------------------------------------------------------\n",
    "- This cell installs only the libraries needed for a fully local pipeline:\n",
    "  pandas/numpy for data handling, transformers/torch for DPR encoders, faiss-cpu\n",
    "  for vector search, and tqdm for progress bars.\n",
    "- Intentionally omits OpenAI and external chat/LLM calls.\n",
    "- Keeps FAISS CPU to avoid faiss-gpu install issues and ensure portability.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "%pip install -q --upgrade pandas numpy tqdm torch transformers faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 1 — IMPORTS, GLOBAL CONFIG, AND SEEDING\n",
    "-------------------------------------------------------------------------------\n",
    "- Imports core libraries and sets deterministic behavior where possible.\n",
    "- Selects device automatically (GPU if available, else CPU).\n",
    "- Defines model names for DPR question and context encoders.\n",
    "- Sets default paths for input dataset and output artifacts.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder,\n",
    "    DPRQuestionEncoderTokenizer,\n",
    "    DPRContextEncoder,\n",
    "    DPRContextEncoderTokenizer,\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DPR model names (stable defaults)\n",
    "QUESTION_ENCODER_NAME = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "CONTEXT_ENCODER_NAME  = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "\n",
    "# IO paths — update INPUT_CSV if needed\n",
    "INPUT_CSV   = \"medquad.csv\"  # if running on Kaggle: \"/kaggle/input/medquad-medical-question-answer-for-ai-research/medquad.csv\"\n",
    "OUTPUT_DIR  = \"artifacts\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Encoding / batching parameters\n",
    "MAX_LENGTH  = 256\n",
    "BATCH_SIZE  = 64\n",
    "DTYPE       = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 2 — LOAD DATASET AND VERIFY COLUMNS\n",
    "-------------------------------------------------------------------------------\n",
    "- Loads the dataset and drops empty rows.\n",
    "- Validates presence of expected columns: 'question', 'answer', 'source', 'focus_area'.\n",
    "- Displays the basic shape to confirm successful load.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "# Load CSV\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Validate columns present\n",
    "required_cols = {\"question\", \"answer\", \"source\", \"focus_area\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52cf8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 3 — LIGHTWEIGHT TEXT PREPROCESSING\n",
    "-------------------------------------------------------------------------------\n",
    "- Performs minimal, robust normalization suitable for DPR encoders:\n",
    "  lowercasing, trimming whitespace, and collapsing repeated spaces.\n",
    "- Creates new columns 'question_clean' and 'answer_clean'.\n",
    "- Keeps original text columns unchanged for traceability.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df[\"question_clean\"] = df[\"question\"].apply(clean_text)\n",
    "df[\"answer_clean\"]   = df[\"answer\"].apply(clean_text)\n",
    "\n",
    "df[[\"question\", \"question_clean\", \"answer\", \"answer_clean\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e610cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 4 — LOAD DPR QUESTION AND CONTEXT ENCODERS\n",
    "-------------------------------------------------------------------------------\n",
    "- Loads the proper tokenizer/encoder pairs for DPR question and context encoders.\n",
    "- Moves models to the chosen device (GPU if available, otherwise CPU).\n",
    "- Uses .eval() and torch.no_grad() for deterministic, inference-only behavior.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "# Question side\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(QUESTION_ENCODER_NAME)\n",
    "q_encoder   = DPRQuestionEncoder.from_pretrained(QUESTION_ENCODER_NAME).to(DEVICE).eval()\n",
    "\n",
    "# Context/answer side\n",
    "c_tokenizer = DPRContextEncoderTokenizer.from_pretrained(CONTEXT_ENCODER_NAME)\n",
    "c_encoder   = DPRContextEncoder.from_pretrained(CONTEXT_ENCODER_NAME).to(DEVICE).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 5 — BATCHED ENCODING UTILITIES\n",
    "-------------------------------------------------------------------------------\n",
    "- Defines a memory-efficient, batched encoding function for arbitrary text arrays.\n",
    "- For DPR models, the pooled CLS representation is accessed via .pooler_output.\n",
    "- Returns a NumPy float32 array of shape [N, D].\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "def encode_texts(texts, tokenizer, model, batch_size=BATCH_SIZE, max_length=MAX_LENGTH, device=DEVICE):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\", unit=\"batch\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            # DPR returns pooler_output as [batch, hidden_size]\n",
    "            pooled = outputs.pooler_output.detach().to(\"cpu\").type(torch.float32).numpy()\n",
    "            embeddings.append(pooled)\n",
    "    return np.vstack(embeddings).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 6 — ENCODE ANSWERS (CONTEXTS) AND QUESTIONS\n",
    "-------------------------------------------------------------------------------\n",
    "- Encodes the cleaned answers with the context encoder (index documents).\n",
    "- Encodes the cleaned questions with the question encoder (optional, saved for QA).\n",
    "- Shapes match DPR hidden size, typically 768.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "answers   = df[\"answer_clean\"].tolist()\n",
    "questions = df[\"question_clean\"].tolist()\n",
    "\n",
    "answer_embeddings   = encode_texts(answers,   c_tokenizer, c_encoder, batch_size=BATCH_SIZE, max_length=MAX_LENGTH, device=DEVICE)\n",
    "question_embeddings = encode_texts(questions, q_tokenizer, q_encoder, batch_size=BATCH_SIZE, max_length=MAX_LENGTH, device=DEVICE)\n",
    "\n",
    "print(\"Answer embeddings:\",   answer_embeddings.shape)\n",
    "print(\"Question embeddings:\", question_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd85272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 7 — BUILD FAISS INDEX (CPU, COSINE VIA NORMALIZED INNER PRODUCT)\n",
    "-------------------------------------------------------------------------------\n",
    "- Normalizes embeddings to unit length so that inner product equals cosine similarity.\n",
    "- Uses IndexFlatIP for high recall and simplicity.\n",
    "- Adds all answer embeddings to the index to enable retrieval.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "# Normalize to use cosine similarity via inner product\n",
    "faiss.normalize_L2(answer_embeddings)\n",
    "\n",
    "dim = answer_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(answer_embeddings)\n",
    "\n",
    "print(f\"FAISS CPU index built with {index.ntotal} vectors at dimension {dim}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 8 — OPTIONAL QUICK SMOKE TEST (NO EXTERNAL LLM)\n",
    "-------------------------------------------------------------------------------\n",
    "- Runs a tiny retrieval check using a random dataset question.\n",
    "- Encodes the sample question, normalizes it, and searches top-k in FAISS.\n",
    "- Prints the top results with basic fields to verify end-to-end correctness.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "do_quick_test = True\n",
    "TOP_K = 5\n",
    "\n",
    "if do_quick_test and len(questions) > 0:\n",
    "    test_q = random.choice(questions)\n",
    "    with torch.no_grad():\n",
    "        qi = q_tokenizer([test_q], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "        qi = {k: v.to(DEVICE) for k, v in qi.items()}\n",
    "        qo = q_encoder(**qi).pooler_output.detach().cpu().numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(qo)\n",
    "    scores, idx = index.search(qo, TOP_K)\n",
    "    idx = idx[0].tolist()\n",
    "    scores = scores[0].tolist()\n",
    "\n",
    "    print(\"\\nQuery:\", test_q[:200], \"...\")\n",
    "    print(\"\\nTop results:\")\n",
    "    for rank, (i, s) in enumerate(zip(idx, scores), 1):\n",
    "        print(f\"- {rank:>2}. score={s:.4f} | focus_area={df.loc[i, 'focus_area']} | source={df.loc[i, 'source']}\")\n",
    "        print(f\"      answer: {df.loc[i, 'answer_clean'][:200]} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29605710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 9 — SAVE EVERYTHING AT THE END\n",
    "-------------------------------------------------------------------------------\n",
    "- Persists all generated artifacts to OUTPUT_DIR:\n",
    "  1) Cleaned dataset as CSV.\n",
    "  2) Numpy .npy files for answer and question embeddings.\n",
    "  3) FAISS index file for CPU.\n",
    "  4) A small config.json with model names, dims, counts, and parameters.\n",
    "  5) A row-to-id mapping CSV for explicit doc_id tracking.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "# 1) Cleaned dataset\n",
    "clean_csv_path = os.path.join(OUTPUT_DIR, \"dataset_clean.csv\")\n",
    "df.to_csv(clean_csv_path, index=False)\n",
    "\n",
    "# 2) Embeddings\n",
    "ans_path = os.path.join(OUTPUT_DIR, \"answer_embeddings.npy\")\n",
    "que_path = os.path.join(OUTPUT_DIR, \"question_embeddings.npy\")\n",
    "np.save(ans_path, answer_embeddings)\n",
    "np.save(que_path, question_embeddings)\n",
    "\n",
    "# 3) FAISS index\n",
    "faiss_path = os.path.join(OUTPUT_DIR, \"faiss_index_cpu.index\")\n",
    "faiss.write_index(index, faiss_path)\n",
    "\n",
    "# 4) Config metadata\n",
    "config = {\n",
    "    \"question_encoder_name\": QUESTION_ENCODER_NAME,\n",
    "    \"context_encoder_name\": CONTEXT_ENCODER_NAME,\n",
    "    \"embedding_dim\": int(answer_embeddings.shape[1]),\n",
    "    \"num_docs\": int(answer_embeddings.shape[0]),\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"max_length\": int(MAX_LENGTH),\n",
    "    \"device\": str(DEVICE),\n",
    "    \"files\": {\n",
    "        \"clean_csv\": clean_csv_path,\n",
    "        \"answer_embeddings\": ans_path,\n",
    "        \"question_embeddings\": que_path,\n",
    "        \"faiss_index_cpu\": faiss_path,\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# 5) Explicit id mapping\n",
    "id_map = pd.DataFrame({\n",
    "    \"doc_id\": np.arange(len(df), dtype=np.int32),\n",
    "    \"source\": df[\"source\"],\n",
    "    \"focus_area\": df[\"focus_area\"],\n",
    "})\n",
    "id_map_path = os.path.join(OUTPUT_DIR, \"id_map.csv\")\n",
    "id_map.to_csv(id_map_path, index=False)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "for k, v in config[\"files\"].items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "print(f\"- id_map: {id_map_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
