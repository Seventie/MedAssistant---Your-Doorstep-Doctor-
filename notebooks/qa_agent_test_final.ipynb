{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "RAG pipeline overview (unaltered logic, comments only)\n",
    "-------------------------------------------------------------------------------\n",
    "This script implements a simple Retrieval-Augmented Generation (RAG) workflow:\n",
    "- Load a preprocessed medical QA dataset and its precomputed answer embeddings. \n",
    "- Normalize document embeddings and build a FAISS index (inner product as cosine).\n",
    "- Load DPR question encoder/tokenizer to embed incoming questions for retrieval.\n",
    "- Retrieve top-k relevant answer passages from FAISS using the question vector.\n",
    "- Compose a prompt with the retrieved context and call a Groq model to generate\n",
    "  an educational answer, returning the model's response.\n",
    "Notes:\n",
    "- The code below is kept identical in logic to the original and annotated using\n",
    "  large triple-quoted comments and brief inline notes for clarity.\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# med_qa_pipeline_groq_clean.py\n",
    "\n",
    "\"\"\"\n",
    "Imports:\n",
    "- numpy/pandas for array and tabular handling\n",
    "- faiss for fast nearest-neighbor search on dense vectors\n",
    "- torch for DPR model execution and device selection (CPU/GPU)\n",
    "- transformers to load DPRQuestionEncoder and tokenizer\n",
    "- groq client for optional answer generation step\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from groq import Groq\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load data and prebuilt embeddings\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Step 1 â€” Data and embeddings:\n",
    "- Reads a preprocessed dataset (expected to contain 'answer_clean')\n",
    "- Loads precomputed document embeddings from disk\n",
    "- L2-normalizes embeddings so that inner product â‰ˆ cosine similarity\n",
    "- Builds an in-memory FAISS IndexFlatIP for retrieval\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"data/medquad_processed.csv\")  # preprocessed dataset\n",
    "docs = df[\"answer_clean\"].astype(str).tolist()\n",
    "\n",
    "encoded_docs = np.load(\"embeddings/encoded_docs.npy\")\n",
    "encoded_docs = encoded_docs / np.linalg.norm(encoded_docs, axis=1, keepdims=True)\n",
    "\n",
    "dimension = encoded_docs.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(encoded_docs)\n",
    "\n",
    "print(\"[INFO] FAISS index built successfully (in-memory).\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load DPR encoders\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Step 2 â€” DPR question encoder:\n",
    "- Selects device automatically (GPU if available, else CPU)\n",
    "- Loads the DPR question encoder and tokenizer for query embedding\n",
    "- These are used to embed incoming questions at inference time\n",
    "\"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"[INIT] Loading DPR encoders...\")\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    \"facebook/dpr-question_encoder-single-nq-base\"\n",
    ").to(device)\n",
    "\n",
    "# NOTE: The following line is intentionally unchanged to preserve original code.\n",
    "# It contains the original tokenizer initialization exactly as provided.\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
    "    \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "\n",
    "print(\"[INFO] DPR encoders loaded successfully.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Setup Groq client\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Step 3 â€” Groq client:\n",
    "- Initializes the Groq client using the provided API key\n",
    "- Used later to generate an educational answer from retrieved context\n",
    "\"\"\"\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Helper functions\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Helper: retrieve_context(question, top_k)\n",
    "- Tokenizes and encodes the question with DPR\n",
    "- L2-normalizes the query embedding\n",
    "- Searches the FAISS index for top_k nearest documents\n",
    "- Returns a single string by concatenating the retrieved answer passages\n",
    "\"\"\"\n",
    "def retrieve_context(question: str, top_k: int = 5):\n",
    "    \"\"\"Retrieve top-k relevant contexts using FAISS.\"\"\"\n",
    "    # Tokenize question for DPR encoder\n",
    "    inputs = question_tokenizer(\n",
    "        question, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Encode question (no gradients needed for inference)\n",
    "    with torch.no_grad():\n",
    "        q_emb = question_encoder(**inputs).pooler_output.cpu().numpy()\n",
    "        # Normalize query to align with cosine-style IP search\n",
    "        q_emb = q_emb / np.linalg.norm(q_emb, axis=1, keepdims=True)\n",
    "\n",
    "    # FAISS search for nearest neighbors\n",
    "    scores, indices = index.search(q_emb, top_k)\n",
    "    retrieved_texts = [docs[i] for i in indices[0]]\n",
    "\n",
    "    # Concatenate retrieved texts into one retrieval context\n",
    "    return \" \".join(retrieved_texts)\n",
    "\n",
    "\"\"\"\n",
    "Helper: generate_answer_groq(question, context)\n",
    "- Crafts a prompt that instructs the model to answer using the retrieved context\n",
    "- Calls Groq chat.completions with a small temperature and token cap\n",
    "- Returns the model's text response; on exception, returns an error string\n",
    "\"\"\"\n",
    "def generate_answer_groq(question: str, context: str) -> str:\n",
    "    \"\"\"Generate factual medical answer using Groq model.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a knowledgeable medical assistant designed for educational and informational purposes only.\n",
    "Your task is to provide clear, factually accurate, and educational answers.\n",
    "Follow these instructions carefully:\n",
    "1. Use the provided context primarily to form your answer.\n",
    "2. If the context does not fully answer the question, provide a brief, logical, and educational explanation using your general medical understanding.\n",
    "3. Indicate which parts of the provided context were most relevant to your answer.\n",
    "4. Do NOT give warnings like \"I cannot provide medical advice\" â€” instead, frame everything as educational information.\n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "Answer (for educational purposes only):\n",
    "\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR calling Groq API] {e}\"\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Main QA function\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"\n",
    "ask_question(question, top_k)\n",
    "- Full pipeline entry:\n",
    "  1) Retrieve top_k contexts using DPR + FAISS\n",
    "  2) Generate an educational answer from the retrieved context via Groq\n",
    "- Returns the final text answer\n",
    "\"\"\"\n",
    "def ask_question(question: str, top_k: int = 5):\n",
    "    \"\"\"Get context, retrieve top docs, and generate answer.\"\"\"\n",
    "    context = retrieve_context(question, top_k=top_k)\n",
    "    answer = generate_answer_groq(question, context)\n",
    "    return answer\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Interactive Run\n",
    "# -------------------------------\n",
    "\n",
    "\"\"\"\n",
    "CLI loop:\n",
    "- Repeatedly reads a question from stdin\n",
    "- Calls ask_question and prints the result\n",
    "- Type 'exit' to terminate the session\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== ðŸ©º Medical QA Assistant (Groq + DPR + FAISS) ===\\n\")\n",
    "    print(\"Type your medical question below. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        question = input(\"ðŸ’¬ Enter your question: \").strip()\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Exiting... Stay healthy! ðŸ«¶\")\n",
    "            break\n",
    "\n",
    "        final_answer = ask_question(question)\n",
    "        print(\"\\nðŸ©º Question:\", question)\n",
    "        print(\"\\nðŸ’¬ Answer:\", final_answer)\n",
    "        print(\"\\n\" + \"-\" * 60 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
